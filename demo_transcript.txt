Hey all, in today's Generative AI demo I'll be covering (2) uses cases I've implemented in Azure Functions. One provides a method of categorizing user reviews using semantic analysis and the other is a concept for verifying model output. Both of them make use of triggers that call these functions when a file, or BLOB, is created within an Azure Storage Account container. Additionally I'll discuss the pros and cons of different LLMs, specifically OpenAI's ChapGPT and Google's Bard. To start I'll show you what I've worked on in an attempt to verify the output of the models, specifically when the model exhibits hallucination and then I'll cover the semantic analysis use case from last week. 

Both use cases have similar implementations shown here in the diagram of the data flow for the applications:
    Both functions will be triggered when a file is loaded into their respective containers.
    The functions will then make one or more API calls to ChatGPT that return results to the function.
    Then, after some formatting in the function, the output will be sent to the respective containers.

For the first use case, I'll show you what I've been working on in an attempt to identify when the models hallucinate information. Hallucination is a common problem in current LLMs that needs to be addressed because until the underlying technology has been improved it will be necessary to validate the information returned from the model. The current solution that I am working on actually draws on the strengths of (2) different LLMs; ChatGPT for content generation, and Bard, Google's LLM, for fact checking. In my tests, and reports from others using these LLMs, ChatGPT performs better in content generation for the information it was trained on compared to Bard, however it is limited in what it can access currently, whereas Bard can perform Google searches to verify information, if used correctly but not in all circumstances. A file with a prompt for content is uploaded to the container, and then function makes a call to ChatGPT for content. After the content it returned, I make use of the Fact Check List Pattern, which is a prompt pattern within prompt engineering, to send the content back to ChatGPT and to ask it to list the facts that are stated within. Once it returns those facts, and after some modification, we can then pass it to Bard to verify.

As a disclaimer, this is an intial concept, and the API for Bard currently is in a limited access beta, so I won't be able to make automated calls to it to verify the facts, but when it becomes publicly available I'll be able to add that functionality. For now though I can pass these facts in manually to demonstrate how Bard has access to current information from Google. I'll show you an example of how Bard can do this, and then I'll get in to the demo.

Moving into the demonstration, I'll show how semantic analysis can be done on user reviews by uploading a file into the container:

So you can see how we can make use of the different strengths in different LLMs in order to get a solution that works for us. As these technologies become more available, like Bards API, there will be many use cases and solutinos that can help our clients do more than ever before.

Thank you