Hey all, in today's Generative AI demo I'll be covering (2) uses cases I've implemented in Azure Functions. One can provide a method of categorizing user reviews using semantic analysis and the other is a concept for verifying model output. Both of them make use of triggers that call these functions when a file, or BLOB, is created within an Azure Storage Account container. Additionally I'll discuss how pros and cons of different LLMS, specifically ChapGPT and Bard. I'll start with the semantic analysis use case and then I'll show you what I've worked on attempting to verify the output of the models, specifically when the model exhibits hallucination.

Both use cases have similar implementations shown here in the diagram of the data flow for the applications:

I'll start with the semantic analysis demonstration here by uploading a file into the container:

For the next use case, I'll show you what I've been working on in an attempt to identify when the models hallucinate information. This is common problem in current LLMs that needs to be addressed because until the underlying technology has been improved it will be necessary to validate the information returned from the model. The current solution that I am working on actually draws on the strengths from (2) different LLMs; ChatGPT for content generation, and Bard, Google's LLM, for fact checking. In my tests, and reports from others using these LLMs, ChatGPT performs better in content generation for the information it was trained on compared to Bard, however it is limited in what it can access currently, whereas Bard can perform Google searches to verify information, if used correctly and not in all circumstances. A file with a prompt for content is uploaded to the container, and the function makes a call to ChatGPT for content. After the content it returned, I make use of the Fact Check List Pattern to send the content back to ChatGPT and to ask it to list the facts that are stated within. Once it returns those facts, and after some modification, we can then pass it to Bard to verify.

As a disclaimer, this is an intial concept, and the API for Bard currently is in a limited access beta, so I won't be able to make automated calls to it to verify the facts, but when it becomes publicly available I'll be able to add that functionality. For now though I can pass these facts in manually to demonstrate how Bard has access to current information from Google.